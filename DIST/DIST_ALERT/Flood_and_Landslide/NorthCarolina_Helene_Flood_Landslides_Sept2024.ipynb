{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying Flood and Landslide Impacts Using the OPERA DIST-HLS Product\n",
    "\n",
    "### This example showcases an application of the OPERA DIST-HLS dataset to visualize and explore land surface disturbance related to flooding and landslides in the Appalachian Mountains (North Carolina, USA) associated with Hurricane Helene (09/24- 09/29/2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow utilized the Leafmap library, which provides a suite of tools for interactive mapping and visualization in Jupyter Notebooks. Leafmap version 0.30.0 and later offer tools specifically for accessing NASA Earthdata by building on the newly developed NASA Earthaccess library. Earthaccess provides streamlined access to NASA Earthdata and simplifies the authentication and querying process over previously developed approaches.This notebook is designed to leverage tools within Earthaccess and Leafmap to facility easier access and vizualization of OPERA data products for a user-specified area of interest (AOI). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPERA info\n",
    "See website https://www.jpl.nasa.gov/go/opera/products/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Notebook dependencies may be installed into a self-contained python environment using the `environment.yml` file available in the [OPERA Applications Github repository](https://github.com/OPERA-Cal-Val/OPERA_Applications) or they may be installed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import leafmap\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and set working directory (modify to your desired path)\n",
    "working_directory = os.path.expanduser('~/opera/disaster_response/hurricane_helene/')\n",
    "os.makedirs(working_directory, exist_ok=True)\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Verify the working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication \n",
    "A [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) account is required to download the data used in this tutorial. You can create an account at the link provided. After establishing an account, the code in the next cell will verify authentication. If this is your first time running the notebook, you will be prompted to enter your Earthdata login credentials, which will be saved in ~/.netrc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leafmap.nasa_data_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View NASA Earthdata datasets\n",
    "A tab separated values (TSV) file, made available through the opengeos Github repository, catalogues metadata for more than 9,000 datasets available through NASA Earthdata. In the next cell we load the TSV into a pandas dataframe and view the metadata for the first five (5) Earthdata products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Earthdata datasets from .tsv file into a pandas dataframe\n",
    "earthdata_url = 'https://github.com/opengeos/NASA-Earth-Data/raw/main/nasa_earth_data.tsv'\n",
    "earthdata_df = pd.read_csv(earthdata_url, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the available OPERA products\n",
    "Note above that the `earthdata_df` contains a number of columns with metadata about each available product. the `ShortName` column will be used to produce a new dataframe containing only OPERA products. Let's view the available products and their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opera_df = earthdata_df[earthdata_df['ShortName'].str.contains('OPERA', case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an area of interest (AOI) and time period of interest (TOI)\n",
    "Below we define an area of interest (AOI) and the time period over which we would like to discover DIST-HLS data. The AOI is selected based off reported flood/landslide impact. We select data from 02-22-2025 in order to explore vegetation loss/disturbance long after the flood water have receded and to allow for multiple Landsat / Sentinel-2 overpasses to constrain areal extent of land surface change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AOI selected based on region of known significant flooding and landsliding associated with Hurricane Helene in Western NC, USA\n",
    "AOI = (-82.80, 35.45, -82.20, 35.83) #W, S, E, N; Western NC, USA\n",
    "\n",
    "#A single Landsat 8 granule acquired on 02/22/2025 captures a region affected by substantial flooding and landslides associated with Hurricane Helene\n",
    "StartDate_PostFlood=\"2025-02-22T00:00:00\"  #Post-flood image start date\n",
    "EndDate_PostFlood=\"2025-02-22T23:59:59\"    #Post-flood image end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Earthdata and return metadata for OPERA products within the AOI\n",
    "The `earthaccess` library makes it simple to quickly query NASA's Common Metadata Repository (CMR) and return the associated metadata as a Geodataframe. `Leafmap` has recently added functionality that builds on `earthaccess` to enable interactive viewing of this data. \n",
    "In the next cell, the user should specify which OPERA product and the date range of interest. The AOI defined previously is used as the boundary in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View OPERA Product Shortnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print the available OPERA datasets \n",
    "print('Available OPERA datasets:', opera_df['ShortName'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the OPERA DIST-ALERT-HLS dataset for the AOI\n",
    "The below query should return a single granule covering a large region near Asheville, North Carolina, USA. It is also possible to return and merge multiple granules to explore a larger area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_results_PostFlood, dist_gdf_PostFlood = leafmap.nasa_data_search(\n",
    "    short_name='OPERA_L3_DIST-ALERT-HLS_V1',\n",
    "    cloud_hosted=True,\n",
    "    bounding_box= AOI,\n",
    "    temporal=(StartDate_PostFlood, EndDate_PostFlood),\n",
    "    count=-1,  # use -1 to return all datasets\n",
    "    return_gdf=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the available DIST-ALERT-HLS layers\n",
    "Functionality within earthaccess enables more more asthetic views of the available layers, as well as displaying the thumbnail. These links are clickable and will download in the browser when clicked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_results_PostFlood[0] #Note this just shows a single MGRS/HLS tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the DIST-ALERT-HLS metadata and footprints\n",
    "We can use `geopandas.explore()` to visualize the footprint of our granule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the location of the tiles \n",
    "dist_gdf_PostFlood.explore(fill=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data with leafmap\n",
    "Let's download the data from one of our above queries. In the cell below we create a directory where the OPERA DIST-HLS data will be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a subdirectory for the OPERA DIST-HLS data\n",
    "This will be the location on your file system where OPERA DIST-HLS files are downloaded. It will be a subdirectory inside of a directory called `data`, and the directory name will be the date that it was created. In this way, you can return to this notebook at a later date, and the data will not overwrite previous data versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_directory(dirname=None):\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "\n",
    "    # Define the base directory\n",
    "    base_directory = \"data\"\n",
    "\n",
    "    # Create the full path for the new directory\n",
    "    if dirname is None:\n",
    "        new_directory_path = os.path.join(base_directory, f\"data_{current_datetime}\")\n",
    "    else:\n",
    "        new_directory_path = os.path.join(base_directory, f\"data_{current_datetime}\", dirname)\n",
    "\n",
    "    # Create the new directory\n",
    "    os.makedirs(new_directory_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Directory '{new_directory_path}' created successfully.\")\n",
    "\n",
    "    return new_directory_path \n",
    "\n",
    "directory_path_PostFlood = create_data_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "1 OPERA DIST-HLS granuels intersect our AOI, each granule consisting of 19 unique data layers. The below will download the data to your newly created subdirectory. Look on your file system for a directory `/data/date` where `date` is the date the directory was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_data_PostFlood = leafmap.nasa_data_download(dist_results_PostFlood, out_dir=directory_path_PostFlood)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to post-event disturbance\n",
    "We would like to view only disturbance detected after Hurricane Helene, in order to pinpoint the regions affected by flooding and landsliding. To do so, we produce a 'filtered' derivative of the DIST-ALERT HLS layers that is constained by the date of intitial disturbance detection (determined by the `VEG-DIST-DATE` layer). Pixel values in the `VEG-DIST-DATE` layer correspond the date of initial disturbance detection in units of days since December 31, 2020. Because we are interested in only post-hurricane disturbance, we filter out disturbance that occurred before this date, corresponding to `VEG-DIST-DATE` pixel values less than 1371 (the number of days spanning the from 12/31/2020 to the first post-flood DIST-HLS product, 10/02/2024).\n",
    "\n",
    "Below we calculate the number of days since December 31, 2020 and store it as a variable, which will be used as input in a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference date and the target date\n",
    "reference_date = datetime(2020, 12, 31)\n",
    "target_date = datetime(2024, 10, 2)\n",
    "\n",
    "# Calculate the difference between the two dates\n",
    "delta = target_date - reference_date\n",
    "\n",
    "# Get the number of days from the timedelta object\n",
    "days_since_reference = delta.days\n",
    "\n",
    "print(\"Number of days:\", days_since_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by time and slope\n",
    "The two cells below defines and executes a series of functions that produces two subdirectories of DIST-HLS data:\n",
    "- (1) The DIST-HLS layers filtered to post-hurricane distubance\n",
    "- (2) The DIST-HLS layers filtered by the date of disturbance detection and by slope.\n",
    "\n",
    "**Note:** Slope filtered layers are time filtered by default.\n",
    "\n",
    "The filtered data are saved in a new subdirectory called `filtered` and within individual subdirectories `time_filtered` and `slope_filtered`.\n",
    "\n",
    "An additional function called `color_filtered_layers()` colorizes the filtered data by their corresponding color schema from the original DIST-HLS layers. This enables direct visualization in GIS software.\n",
    "\n",
    "Docstrings are provided to aid in the user's understanding of each function's purpose and internal functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a slope threshold\n",
    "Modify the value of `slope_threshold` in the cell below. DIST-HLS pixels with corresponding slopes less than this value will be discarded in the resulting slope filtered DIST-HLS product. Default value of `slope_threshold` is 20 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_threshold = 20 # degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_layers(directory_path_PostFlood):\n",
    "    \"\"\"Merge the layers in the data directory and save the merged rasters. This function is necessary to produce a mosaicked product from multiple granules, if needed.\n",
    "      This will produce merged rasters for each DIST-HLS layer in a subdirectory called 'merged'.\n",
    "      :param directory_path_PostFlood: The directory containing the downloaded HLS data.\n",
    "      :return layer_dict: Dictionary of lists of filenames for each DIST-HLS layer\n",
    "      :return merged_dir: Merged directory path\n",
    "    \"\"\"\n",
    "    # Create a directory for merged rasters\n",
    "    merged_dir = os.path.join(directory_path_PostFlood, 'merged')\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to hold lists of filenames for each layer\n",
    "    layer_dict = {}\n",
    "\n",
    "    # Iterate through filtered rasters to populate the dictionary\n",
    "    for filename in os.listdir(directory_path_PostFlood):\n",
    "        if filename.endswith('.tif'):\n",
    "            # Split the filename to extract the layer name\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 6:  # Check if there are enough parts to avoid index errors\n",
    "                layer_name = parts[-1].replace('.tif', '') # Extract the unique layer name (last part)\n",
    "            if layer_name not in layer_dict:\n",
    "                layer_dict[layer_name] = []\n",
    "            layer_dict[layer_name].append(os.path.join(directory_path_PostFlood, filename))\n",
    "\n",
    "    # Merge rasters for each layer and save them\n",
    "    for layer_name, files in layer_dict.items():\n",
    "        # Open the rasters and extract nodata values\n",
    "        src_files_to_mosaic = [rasterio.open(f) for f in files]\n",
    "        \n",
    "        # Get the consistent nodata value for the layer\n",
    "        nodata_value = src_files_to_mosaic[0].nodata\n",
    "        \n",
    "        # Perform the merge with preference for data pixels\n",
    "        mosaic, out_trans = merge(src_files_to_mosaic, nodata=nodata_value, method='first')\n",
    "        \n",
    "        # Create metadata for the merged raster\n",
    "        out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": mosaic.shape[1],\n",
    "            \"width\": mosaic.shape[2],\n",
    "            \"transform\": out_trans,\n",
    "            \"nodata\": nodata_value\n",
    "        })\n",
    "\n",
    "        # Save the merged raster\n",
    "        merged_filename = f\"{layer_name}_merged.tif\"\n",
    "        merged_filepath = os.path.join(merged_dir, merged_filename)\n",
    "\n",
    "        with rasterio.open(merged_filepath, 'w', **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "\n",
    "        print(f\"Merged raster saved as: {merged_filename}\")\n",
    "\n",
    "        # Close all opened raster files\n",
    "        for src in src_files_to_mosaic:\n",
    "            src.close()\n",
    "\n",
    "    return layer_dict, merged_dir\n",
    "\n",
    "def generate_filtered_rasters(date_threshold, slope_threshold=20, slope_filter = False):\n",
    "    \"\"\"Generate a filtered version of each raster based on date/slope thresholds. Date threshold is the number of days since the reference date.\n",
    "    Production of slope filtered rasters is optional. Filtered files are stored in subdirectories within the 'merged/filtered' directory.\n",
    "    If 'slope_filter' is True, additional filtered rasters are generated based on a slope threshold.\n",
    "    :param date_threshold: The number of days since the reference date to filter the data.\n",
    "    :param slope_threshold: The slope threshold value to use for filtering. Default is 20 degrees.\n",
    "    :param slope_filter: If True, additional filtered rasters are generated based on a slope threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge layers\n",
    "    layer_dict, merged_dir = merge_layers(directory_path_PostFlood)\n",
    "\n",
    "    # Ensure the output subdirectory exists\n",
    "    filtered_dir = 'filtered'\n",
    "    time_filtered_dirname = 'time_filtered'\n",
    "    time_filtered_dir = os.path.join(merged_dir, filtered_dir, time_filtered_dirname)\n",
    "    os.makedirs(time_filtered_dir, exist_ok=True)\n",
    "    generate_time_filtered_rasters(merged_dir, time_filtered_dir, date_threshold)\n",
    "    color_filtered_layers(layer_dict, time_filtered_dir)\n",
    "\n",
    "    if slope_filter:\n",
    "        slope_filtered_dirname = 'slope_filtered'\n",
    "        slope_filtered_dir = os.path.join(merged_dir, filtered_dir, slope_filtered_dirname)\n",
    "        os.makedirs(slope_filtered_dir, exist_ok=True)\n",
    "        generate_slope_filtered_rasters(time_filtered_dir, slope_filtered_dir, slope_threshold)\n",
    "        color_filtered_layers(layer_dict, slope_filtered_dir)\n",
    "    return\n",
    "\n",
    "def generate_time_filtered_rasters(merged_dir, time_filtered_dir, date_threshold):\n",
    "    \"\"\"Generate a filtered version of each raster based on the date threshold. \n",
    "    These files are stored in subdirectory called 'time_filtered' within the 'merged/filtered' directory.\n",
    "    :param merged_dir: The directory containing the merged rasters.\n",
    "    :param time_filtered_dir: The directory to save the time-filtered rasters.\n",
    "    :param date_threshold: The number of days since the reference date to filter the data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process each merged layer\n",
    "    date_file = 'VEG-DIST-DATE_merged.tif'\n",
    "    date_file_path = os.path.join(merged_dir, date_file)\n",
    "    with rasterio.open(date_file_path) as src:\n",
    "        date_data = src.read(1)  # Read the first (and only) band\n",
    "        date_mask = date_data >= date_threshold  # Mask where date data exceeds the date_threshold\n",
    "        \n",
    "    # Apply the mask to each layer file\n",
    "    for file in os.listdir(merged_dir):\n",
    "        if not file.endswith('.tif'):\n",
    "            continue\n",
    "        \n",
    "        print(\"working on file:\", file)\n",
    "        # If the file is _VEG-DIST-DATE.tif, apply the date_threshold and save a filtered version\n",
    "        if file == date_file:\n",
    "            with rasterio.open(date_file_path) as src:\n",
    "                date_filtered_data = np.where(date_mask, date_data, src.nodata)  # Apply the mask\n",
    "                date_filtered_filename = file.replace('.tif', '_filtered.tif')  # Update filename\n",
    "                date_filtered_path = os.path.join(time_filtered_dir, date_filtered_filename)\n",
    "                \n",
    "                # Save the filtered _VEG-DIST-DATE.tif raster\n",
    "                src_meta = src.meta\n",
    "                src_meta.update({\"nodata\": src.nodata})\n",
    "                with rasterio.open(date_filtered_path, 'w', **src_meta) as dest:\n",
    "                    dest.write(date_filtered_data, 1)  # Write to the first band\n",
    "                    print(f\"Generated filtered file: {date_filtered_filename}\")\n",
    "\n",
    "        # If file is _DATA-MASK.tif, copy it directly to the output directory with \"_filtered\" added\n",
    "        elif file.endswith('DATA-MASK_merged.tif'):\n",
    "            data_mask_filtered_filename = file.replace('.tif', '_time_filtered.tif')\n",
    "            data_mask_filtered_path = os.path.join(time_filtered_dir, data_mask_filtered_filename)\n",
    "            print(f\"Copied _DATA-MASK file: {data_mask_filtered_filename}\")\n",
    "            shutil.copy(os.path.join(merged_dir, file), data_mask_filtered_path)\n",
    "            print(f\"Copied _DATA-MASK file: {data_mask_filtered_filename}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            # Open the layer file\n",
    "            file_path = os.path.join(merged_dir, file)\n",
    "            with rasterio.open(file_path) as src:\n",
    "                layer_data = src.read(1)  # Read the first band\n",
    "                layer_meta = src.meta  # Metadata to use for the output file\n",
    "                layer_nodata = src.nodata  # Get the 'nan' value for this layer\n",
    "\n",
    "                # Apply the mask: where date_mask is False, set layer_data to layer_nodata\n",
    "                filtered_data = np.where(date_mask, layer_data, layer_nodata)\n",
    "\n",
    "                # Update the filename to include \"_filtered\"\n",
    "                filtered_filename = file.replace('.tif', '_time_filtered.tif')\n",
    "                filtered_file_path = os.path.join(time_filtered_dir, filtered_filename)\n",
    "\n",
    "                # Save the filtered raster with the same metadata\n",
    "                layer_meta.update({\"nodata\": layer_nodata})\n",
    "                with rasterio.open(filtered_file_path, 'w', **layer_meta) as dest:\n",
    "                    dest.write(filtered_data, 1)  # Write to the first band\n",
    "                    print(f\"Generated filtered file: {filtered_filename}\")\n",
    "\n",
    "def generate_slope_filtered_rasters(time_filtered_dir, slope_filtered_dir, slope_threshold=20):\n",
    "    \"\"\"Generate a filtered version of each raster based a slope mask. Slope is derived from COP30 DEM.\n",
    "    These files are stored in subdirectory called 'slope_filtered' within the 'merged/filtered' directory.\n",
    "    :param time_filtered_dir: The directory containing the time-filtered rasters.\n",
    "    :param slope_filtered_dir: The directory to save the slope-filtered rasters.\n",
    "    :param slope_threshold: The slope threshold value to use for filtering. Default is 20 degrees.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the COP30 DEM data (band 10 of OPERA DSWx-HLS dataset)\n",
    "    dem_results, dem_gdf = leafmap.nasa_data_search(\n",
    "        short_name='OPERA_L3_DSWX-HLS_V1',\n",
    "        cloud_hosted=True,\n",
    "        bounding_box= AOI,\n",
    "        temporal=(StartDate_PostFlood, EndDate_PostFlood),\n",
    "        count=1,  # return the first granule\n",
    "        return_gdf=True,\n",
    "    )\n",
    "    dem_directory = create_data_directory(dirname='DEM')\n",
    "    leafmap.nasa_data_download(dem_results, out_dir=dem_directory)\n",
    "\n",
    "    # Delete unwanted layers, retain DEM.tif\n",
    "    for filename in os.listdir(dem_directory):\n",
    "        file_path = os.path.join(dem_directory, filename)\n",
    "        if os.path.isfile(file_path) and not filename.endswith(\"DEM.tif\"):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "    # Make merged directory\n",
    "    merged_dir = os.path.join(dem_directory, \"merged\")\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "\n",
    "    # Create a list of DEM files for the mosaic\n",
    "    dem_files = [os.path.join(dem_directory, f) for f in os.listdir(dem_directory) if f.endswith(\"DEM.tif\")]\n",
    "\n",
    "    # Define the output mosaic file path\n",
    "    output_mosaic = os.path.join(merged_dir, \"mosaic.tif\")\n",
    "\n",
    "    # Get the UTM EPSG code from the first DEM file\n",
    "    with rasterio.open(dem_files[0]) as src:\n",
    "        utm_epsg = f\"EPSG:{src.crs.to_epsg()}\"\n",
    "\n",
    "    # Merge the DEMs, assigning the UTM zone of the first file in dem_file list\n",
    "    gdal.Warp(\n",
    "        output_mosaic,\n",
    "        dem_files,\n",
    "        format=\"GTiff\",\n",
    "        dstSRS=utm_epsg  # Target projection: UTM Zone 17N\n",
    "    )\n",
    "\n",
    "    print(\"Reprojected mosaic created successfully and saved in:\", output_mosaic)\n",
    "\n",
    "    # Make new directory for slope output\n",
    "    slope_directory = create_data_directory(dirname='SLOPE')\n",
    "\n",
    "    # Generate the slope using GDAL\n",
    "    gdal.DEMProcessing(slope_directory+'/slope.tif', dem_directory+'/merged/'+'mosaic.tif', 'slope', format='GTiff', computeEdges=True)\n",
    "    print(f\"Slope output saved to: {slope_directory}\")\n",
    "    \n",
    "    # Open the slope file\n",
    "    slope_file_path = os.path.join(slope_directory, 'slope.tif')\n",
    "    print(\"Slope file path:\", slope_file_path)\n",
    "\n",
    "    with rasterio.open(slope_file_path) as src:\n",
    "        slope_data = src.read(1)\n",
    "        slope_mask = slope_data >= slope_threshold  # Mask where slope data is less than or equal to threshold\n",
    "\n",
    "    # Perform the slope filtering\n",
    "    for file in os.listdir(time_filtered_dir):\n",
    "\n",
    "        # Open the layer file\n",
    "        file_path = os.path.join(time_filtered_dir, file)\n",
    "        filtered_filename = file.replace('merged_time_filtered.tif', 'merged_time_and_slope_filtered.tif')\n",
    "        filtered_file_path = os.path.join(slope_filtered_dir, filtered_filename)\n",
    "\n",
    "        # If the file ends with _DATA-MASK.tif, copy it directly to the output directory\n",
    "        if file.endswith('DATA-MASK_merged_time_filtered.tif'):\n",
    "            shutil.copy(file_path, filtered_file_path)\n",
    "            print(f\"Copied _DATA-MASK file: {filtered_filename}\")\n",
    "            continue  # Move to the next file in the list\n",
    "\n",
    "        else:\n",
    "            with rasterio.open(file_path) as src:\n",
    "                layer_data = src.read(1)\n",
    "                layer_meta = src.meta\n",
    "                layer_nodata = src.nodata\n",
    "                filtered_data = np.where(slope_mask, layer_data, layer_nodata)\n",
    "\n",
    "                layer_meta.update({\"nodata\": layer_nodata})\n",
    "\n",
    "                with rasterio.open(filtered_file_path, 'w', **layer_meta) as dest:\n",
    "                    dest.write(filtered_data, 1)\n",
    "\n",
    "                print(f\"Generated slope filtered file: {filtered_filename}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def color_filtered_layers(layer_dict, filtered_dir):\n",
    "    \"\"\"Colorize the filtered rasters using the symbology from the original HLS data.\n",
    "    The files are colorized in place within the time/slope filtered directories.\n",
    "    :param layer_dict: Dictionary of lists of filenames for each DIST-HLS layer\n",
    "    :param filtered_dir: The directory containing the filtered rasters.\n",
    "    \"\"\"\n",
    "    symbology_layers = {}\n",
    "\n",
    "    # Loop over each layer in the layer_dict\n",
    "    for layer in layer_dict:\n",
    "        # Check if we already found a file for this layer\n",
    "        if layer not in symbology_layers:\n",
    "            # Loop over each file in the directory\n",
    "            for filename in os.listdir(directory_path_PostFlood):\n",
    "                # Check if the file is a .tif file\n",
    "                if filename.endswith('.tif'):\n",
    "                    # Extract the layer name (last part before the extension)\n",
    "                    layer_name = filename.split('_')[-1].split('.')[0]\n",
    "                    \n",
    "                    # Check if the layer name matches the current layer\n",
    "                    if layer_name == layer:\n",
    "                        # Save the full file path for the first match\n",
    "                        symbology_layers[layer] = os.path.join(directory_path_PostFlood, filename)\n",
    "                        break  # Stop once the first file for this layer is found\n",
    "\n",
    "    for file in os.listdir(filtered_dir):\n",
    "        parts = file.split('_')\n",
    "        layer_name = parts[0]\n",
    "        print(f\"Layer {layer_name}: {symbology_layers.get(layer_name)}\")\n",
    "        try:\n",
    "            # Read the reference symbology raster\n",
    "            with rasterio.open(symbology_layers.get(layer_name)) as src:\n",
    "\n",
    "                # Check if the symbology raster has a colormap\n",
    "                if 1 in src.colormap(1):\n",
    "                    print(f\"Colormap found for {symbology_layers.get(layer_name)}\")\n",
    "                    src_colormap = src.colormap(1)  # Assuming symbology is in band 1\n",
    "                else:\n",
    "                    print(f\"No colormap found for {symbology_layers.get(layer)}\")\n",
    "                    return  # Exit if no colormap exists\n",
    "\n",
    "            # Open the merged raster in write mode\n",
    "            filename = os.path.join(filtered_dir, file)\n",
    "            print(f\"Opening file: {filename}\")\n",
    "            with rasterio.open(filename, 'r+') as dst:\n",
    "                # Write the color map to the first band\n",
    "                dst.write_colormap(1, src_colormap)\n",
    "                print(f\"Colormap written to: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Symbology not present for {symbology_layers.get(layer_name)}: {e}...skipping\")\n",
    "\n",
    "generate_filtered_rasters(days_since_reference, slope_threshold, slope_filter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View one of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a map and add the merged VEG-ANOM-MAX filtered raster\n",
    "merged_dir = os.path.join(directory_path_PostFlood, 'merged')\n",
    "filtered_dir = 'filtered'\n",
    "time_filtered_dirname = 'time_filtered'\n",
    "time_filtered_dir = os.path.join(merged_dir, filtered_dir, time_filtered_dirname)\n",
    "filename = 'VEG-ANOM-MAX_merged_time_filtered.tif'\n",
    "m = leafmap.Map(basemap=\"Esri.WorldImagery\")\n",
    "m.add_raster(os.path.join(merged_dir, filtered_dir, time_filtered_dirname, filename), opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "The filtered DIST-HLS data are now available on your filesystem for use in GIS software or for further python post-processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opera_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
